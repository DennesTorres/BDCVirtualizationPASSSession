{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python",
            "version": "3.6.6",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "##  **HDFS Tiering in a notebook sample**\n",
                "\n",
                "  \n",
                "\n",
                "This sample illustrates how to make a mount in the HDFS environment of a Big Data Cluster\n",
                "\n",
                "The notebook is in Python 3, but you may notice everything is related with command line statements"
            ],
            "metadata": {
                "azdata_cell_guid": "8633dc16-1d7c-4145-ae67-3ebba9ecc74a"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "##  **Prepare Environment Variables**"
            ],
            "metadata": {
                "azdata_cell_guid": "ed1a9bdc-6705-473d-8e4f-687b8c984b98"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import os\r\n",
                "os.environ[\"MOUNT_CREDENTIALS\"]=\"fs.azure.abfs.datalakedemomalta=datalakedemomalta.dfs.core.windows.net,fs.azure.account.key.datalakedemomalta.dfs.core.windows.net=Q3jjCUbUpRRlY/XFKUaMabyl25llyrXgMgJaXBI9ysWqdHKDy22JdnAGIbPN6xGd7oSsTDJwjsd+KPo10cwulg==\"\r\n",
                "os.environ[\"AZDATA_PASSWORD\"]=\"MySQLBigData2019\""
            ],
            "metadata": {
                "azdata_cell_guid": "03d96507-fc99-49a3-9af1-829925204b3f",
                "tags": []
            },
            "outputs": [],
            "execution_count": 4
        },
        {
            "cell_type": "markdown",
            "source": [
                "##  **Login on the cluster**"
            ],
            "metadata": {
                "azdata_cell_guid": "9f76e8da-78b7-444c-a55c-d7443db11d57"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "!azdata login -e https://20.54.90.1:30080 -u admin -n \"sqlbigdata\" -a \"yes\""
            ],
            "metadata": {
                "azdata_cell_guid": "29df9695-1f2e-4415-8da9-6bf794b5b13f"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Logged in successfully to `https://20.54.90.1:30080` in namespace `sqlbigdata`. Setting active context to `sqlbigdata`.\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 5
        },
        {
            "cell_type": "markdown",
            "source": [
                "## **Mount ADLS gen2 storage**\n",
                "\n",
                "The difference between the many storages we can mount on BDC HDFS is the protocol used. In this example I'm using **abfs**\n",
                "\n",
                "AZDATA statement makes asynchronous calls to AKS/BDC API's. The statement may finish without the operation being completed."
            ],
            "metadata": {
                "azdata_cell_guid": "67c6a85f-ecbc-4cbb-b23a-fee67b66e0c5"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "!azdata bdc hdfs mount create --remote-uri abfs://samplecontainer@datalakedemomalta.dfs.core.windows.net/holidays --mount-path /holidays"
            ],
            "metadata": {
                "azdata_cell_guid": "cb2f75cc-d7d6-4c4d-b644-07433b46f4d2"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Create mount for /holidays submitted successfully. Check mount status for progress",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 6
        },
        {
            "cell_type": "markdown",
            "source": [
                "## **Check the Status of the Mount**\n",
                "\n",
                "It could include --mount-path to check the status of only one, but this will only become useful if you have really a lot of mounts\n",
                "\n",
                "Mounts with a huge amount of files may be stuck in _\"Creating\"_ status for some time while the cache is created."
            ],
            "metadata": {
                "azdata_cell_guid": "73eb489e-b704-4e3e-bde7-52cdd5367cac"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "!azdata bdc hdfs mount status"
            ],
            "metadata": {
                "azdata_cell_guid": "a5e402e1-28f0-4e42-8e7b-a8af1e96d884"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "[\n  {\n    \"mount\": \"/holidays\",\n    \"remote\": \"abfs://samplecontainer@datalakedemomalta.dfs.core.windows.net/holidays\",\n    \"state\": \"Ready\"\n  }\n]\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 7
        }
    ]
}